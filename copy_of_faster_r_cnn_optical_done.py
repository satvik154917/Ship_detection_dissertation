# -*- coding: utf-8 -*-
"""Copy of faster_R_CNN_OPTICAL DONE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U0k_gvsf-QRfHg76gKMOl1fbRkM_nNC4
"""

# Complete Faster R-CNN Pipeline for Optical Ship Detection

import os
import shutil
import zipfile
import xml.etree.ElementTree as ET
import numpy as np
import matplotlib.pyplot as plt
import cv2
from PIL import Image
import time

import torch
import torchvision
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from torchvision.transforms import functional as F
from torch.utils.data import Dataset, DataLoader
import torch.optim as optim

print("COMPLETE FASTER R-CNN SHIP DETECTION PIPELINE")
print("=" * 60)
print("Starting from dataset extraction to model training")

# Step 1: Mount Google Drive
print("\nMounting Google Drive...")
from google.colab import drive
drive.mount('/content/drive')
print("Google Drive mounted successfully!")

# Global configuration
ZIP_NAME = "Optical_yolo.zip"
EXTRACT_PATH = "/content/ship_dataset_extracted"

# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

def find_and_extract_dataset():
    """Find and extract the ship dataset ZIP file"""
    print("\nFINDING AND EXTRACTING DATASET")
    print("=" * 50)

    # Look for ZIP file in Google Drive
    possible_paths = [
        f"/content/drive/MyDrive/{ZIP_NAME}",
        f"/content/drive/MyDrive/optical_yolo.zip",
        f"/content/drive/MyDrive/Optical_yolo.zip",
        f"/content/{ZIP_NAME}"
    ]

    zip_path = None
    for path in possible_paths:
        if os.path.exists(path):
            zip_path = path
            print(f"Found ZIP: {path}")
            break

    if not zip_path:
        # Search all ZIP files in Google Drive
        drive_root = "/content/drive/MyDrive"
        if os.path.exists(drive_root):
            all_files = os.listdir(drive_root)
            zip_files = [f for f in all_files if f.endswith('.zip')]
            print(f"Available ZIP files: {zip_files}")

            # Look for optical/yolo files
            for zip_file in zip_files:
                if any(keyword in zip_file.lower() for keyword in ['optical', 'yolo', 'ship']):
                    zip_path = os.path.join(drive_root, zip_file)
                    print(f"Using: {zip_path}")
                    break

    if not zip_path:
        print("Dataset ZIP file not found!")
        print("Please upload your ZIP file to Google Drive root folder")
        print("Expected names: Optical_yolo.zip, optical_yolo.zip, etc.")
        return None

    # Extract ZIP
    print(f"Extracting to: {EXTRACT_PATH}")

    # Remove existing extraction
    if os.path.exists(EXTRACT_PATH):
        shutil.rmtree(EXTRACT_PATH)

    try:
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(EXTRACT_PATH)

        print("Extraction completed!")

        # Show what was extracted
        extracted_items = os.listdir(EXTRACT_PATH)
        print(f"Extracted items: {extracted_items}")

        return EXTRACT_PATH

    except Exception as e:
        print(f"Extraction failed: {e}")
        return None

def find_dataset_structure(extract_path):
    """Find dataset folders with images and annotations"""
    print("\nFINDING DATASET STRUCTURE")
    print("=" * 40)

    images_dir = None
    annotations_dir = None

    # Search for JPEGImages and annotations folders
    for root, dirs, files in os.walk(extract_path):
        # Skip macOS metadata
        if '__MACOSX' in root:
            continue

        # Look for image and annotation directories
        if 'JPEGImages' in dirs:
            potential_images = os.path.join(root, 'JPEGImages')
            potential_annotations = os.path.join(root, 'annotations')

            if os.path.exists(potential_annotations):
                # Count files
                img_files = [f for f in os.listdir(potential_images) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
                xml_files = [f for f in os.listdir(potential_annotations) if f.lower().endswith('.xml')]

                if len(img_files) > 0 and len(xml_files) > 0:
                    images_dir = potential_images
                    annotations_dir = potential_annotations
                    print("Found dataset:")
                    print(f"Images: {images_dir} ({len(img_files)} files)")
                    print(f"Annotations: {annotations_dir} ({len(xml_files)} files)")
                    break

    return images_dir, annotations_dir

def parse_xml_annotation(xml_path):
    """Parse XML annotation and return bounding boxes and labels"""
    try:
        tree = ET.parse(xml_path)
        root = tree.getroot()

        boxes = []
        labels = []

        for obj in root.findall('object'):
            # Get bounding box coordinates
            bbox = obj.find('bndbox')
            xmin = float(bbox.find('xmin').text)
            ymin = float(bbox.find('ymin').text)
            xmax = float(bbox.find('xmax').text)
            ymax = float(bbox.find('ymax').text)

            # Ensure valid bounding box
            if xmax > xmin and ymax > ymin:
                boxes.append([xmin, ymin, xmax, ymax])
                labels.append(1)  # All objects are ships (class 1, background is 0)

        return boxes, labels
    except Exception as e:
        print(f"Error parsing {xml_path}: {e}")
        return [], []

class OpticalShipDataset(Dataset):
    """Dataset for optical ship images"""

    def __init__(self, images_dir, annotations_dir):
        self.images_dir = images_dir
        self.annotations_dir = annotations_dir

        # Get all image files with corresponding annotations
        self.image_files = []
        for f in os.listdir(images_dir):
            if f.lower().endswith(('.jpg', '.jpeg', '.png')):
                # Check if corresponding XML exists
                base_name = os.path.splitext(f)[0]
                xml_path = os.path.join(annotations_dir, base_name + '.xml')
                if os.path.exists(xml_path):
                    self.image_files.append(f)

        print(f"Created dataset with {len(self.image_files)} valid image-annotation pairs")

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        # Load image
        img_name = self.image_files[idx]
        img_path = os.path.join(self.images_dir, img_name)

        # Load image as RGB (optical images)
        image = Image.open(img_path).convert('RGB')
        width, height = image.size

        # Load annotation
        base_name = os.path.splitext(img_name)[0]
        xml_path = os.path.join(self.annotations_dir, base_name + '.xml')

        boxes, labels = parse_xml_annotation(xml_path)

        # Handle empty annotations
        if len(boxes) == 0:
            boxes = torch.zeros((0, 4), dtype=torch.float32)
            labels = torch.zeros((0,), dtype=torch.int64)
        else:
            # Clip boxes to image boundaries
            boxes = np.array(boxes)
            boxes[:, 0] = np.clip(boxes[:, 0], 0, width)
            boxes[:, 1] = np.clip(boxes[:, 1], 0, height)
            boxes[:, 2] = np.clip(boxes[:, 2], 0, width)
            boxes[:, 3] = np.clip(boxes[:, 3], 0, height)

            boxes = torch.as_tensor(boxes, dtype=torch.float32)
            labels = torch.as_tensor(labels, dtype=torch.int64)

        # Convert image to tensor
        image = F.to_tensor(image)

        # Create target dictionary
        target = {
            'boxes': boxes,
            'labels': labels,
            'image_id': torch.tensor([idx], dtype=torch.int64),
        }

        return image, target

def collate_fn(batch):
    """Custom collate function for DataLoader"""
    return tuple(zip(*batch))

def create_model(num_classes=2):
    """Create Faster R-CNN model with pretrained backbone"""
    # Use pretrained Faster R-CNN with ResNet-50 backbone
    model = fasterrcnn_resnet50_fpn(pretrained=True)

    # Replace the classifier head for our number of classes
    in_features = model.roi_heads.box_predictor.cls_score.in_features
    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(
        in_features, num_classes
    )

    return model

def train_one_epoch(model, optimizer, data_loader, device, epoch):
    """Train for one epoch"""
    model.train()
    total_loss = 0
    num_batches = 0

    for batch_idx, (images, targets) in enumerate(data_loader):
        # Move to device
        images = list(image.to(device) for image in images)
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

        # Forward pass
        loss_dict = model(images, targets)
        losses = sum(loss for loss in loss_dict.values())

        # Check for invalid loss
        if not torch.isfinite(losses):
            print(f"Warning: Non-finite loss at batch {batch_idx}")
            continue

        # Backward pass
        optimizer.zero_grad()
        losses.backward()
        optimizer.step()

        total_loss += losses.item()
        num_batches += 1

        # Print progress every 10 batches
        if batch_idx % 10 == 0:
            print(f"Batch {batch_idx}/{len(data_loader)}, Loss: {losses.item():.4f}")

    return total_loss / num_batches if num_batches > 0 else 0

def main_pipeline():
    """Complete pipeline from extraction to training"""

    # Step 1: Extract dataset
    print("STEP 1: DATASET EXTRACTION")
    extract_path = find_and_extract_dataset()
    if not extract_path:
        print("Cannot proceed without dataset extraction!")
        return None, None

    # Step 2: Find dataset structure
    print("\nSTEP 2: DATASET STRUCTURE")
    images_dir, annotations_dir = find_dataset_structure(extract_path)
    if not images_dir or not annotations_dir:
        print("Dataset structure not found!")
        return None, None

    # Step 3: Create dataset
    print("\nSTEP 3: DATASET CREATION")
    dataset = OpticalShipDataset(images_dir, annotations_dir)

    if len(dataset) == 0:
        print("No valid samples found!")
        return None, None

    # Split dataset (80% train, 20% validation)
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(
        dataset, [train_size, val_size],
        generator=torch.Generator().manual_seed(42)
    )

    print(f"Dataset split: {train_size} training, {val_size} validation")

    # Step 4: Create data loaders
    print("\nSTEP 4: DATA LOADERS")
    train_loader = DataLoader(
        train_dataset,
        batch_size=4,
        shuffle=True,
        collate_fn=collate_fn,
        num_workers=2,
        pin_memory=True if torch.cuda.is_available() else False
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=2,
        shuffle=False,
        collate_fn=collate_fn,
        num_workers=2,
        pin_memory=True if torch.cuda.is_available() else False
    )

    print("Data loaders created")

    # Step 5: Create model
    print("\nSTEP 5: MODEL CREATION")
    model = create_model(num_classes=2)  # Background + ship
    model.to(device)
    print(f"Faster R-CNN model created and moved to {device}")

    # Step 6: Setup training
    print("\nSTEP 6: TRAINING SETUP")
    params = [p for p in model.parameters() if p.requires_grad]
    optimizer = optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)
    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)
    print("Optimizer and scheduler configured")

    # Step 7: Training
    print("\nSTEP 7: MODEL TRAINING")
    num_epochs = 10
    train_losses = []

    print(f"Starting training for {num_epochs} epochs...")
    start_time = time.time()

    for epoch in range(num_epochs):
        print(f"\nEpoch {epoch + 1}/{num_epochs}")
        print("-" * 30)

        # Train
        epoch_loss = train_one_epoch(model, optimizer, train_loader, device, epoch)
        train_losses.append(epoch_loss)

        # Update learning rate
        lr_scheduler.step()

        print(f"Epoch {epoch + 1} completed - Average Loss: {epoch_loss:.4f}")

    training_time = time.time() - start_time
    print(f"\nTraining completed in {training_time:.2f} seconds!")

    # Step 8: Save model
    print("\nSTEP 8: SAVING MODEL")
    model_path = "/content/drive/MyDrive/faster_rcnn_ship_detector.pth"
    torch.save(model.state_dict(), model_path)
    print(f"Model saved to: {model_path}")

    # Step 9: Plot training history
    plt.figure(figsize=(10, 6))
    plt.plot(range(1, len(train_losses) + 1), train_losses, 'b-o', linewidth=2, markersize=6)
    plt.title('Faster R-CNN Training Loss', fontsize=14, fontweight='bold')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()

    # Save plot
    plot_path = "/content/drive/MyDrive/training_loss_plot.png"
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.show()
    print(f"Training plot saved to: {plot_path}")

    # Make model and dataset available globally
    globals()['trained_model'] = model
    globals()['val_dataset'] = val_dataset
    globals()['dataset_info'] = {
        'total_samples': len(dataset),
        'train_samples': train_size,
        'val_samples': val_size,
        'training_time': training_time,
        'final_loss': train_losses[-1] if train_losses else 0
    }

    print("\nPIPELINE COMPLETED SUCCESSFULLY!")
    print("=" * 50)
    print(f"Total samples: {len(dataset)}")
    print(f"Training samples: {train_size}")
    print(f"Validation samples: {val_size}")
    print(f"Training time: {training_time:.2f} seconds")
    print(f"Final loss: {train_losses[-1]:.4f}")
    print("Model available as 'trained_model'")
    print("Validation dataset available as 'val_dataset'")
    print("Ready for visualization!")

    return model, val_dataset

# Run the complete pipeline
if __name__ == "__main__":
    print("STARTING COMPLETE PIPELINE...")
    result = main_pipeline()

    if result[0] is not None:
        print("\nSUCCESS! You can now run the visualization script.")
    else:
        print("\nPipeline failed. Please check the error messages above.")

# Visualization Script - Run after the complete pipeline

def visualize_ship_detection(model, dataset, num_images=6, confidence_threshold=0.5):
    """Visualize ship detection results"""
    model.eval()

    # Get random samples
    indices = torch.randperm(len(dataset))[:num_images]

    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    axes = axes.ravel()

    colors = {
        'ground_truth': (0, 255, 0),  # Green
        'prediction': (255, 0, 0),    # Red
    }

    with torch.no_grad():
        for i, ax in enumerate(axes):
            if i >= len(indices):
                ax.axis('off')
                continue

            # Get sample
            img_idx = indices[i].item()
            image, target = dataset[img_idx]

            # Make prediction
            image_tensor = image.unsqueeze(0).to(device)
            prediction = model(image_tensor)[0]

            # Convert image for visualization
            img_np = image.cpu().numpy().transpose(1, 2, 0)
            img_np = (img_np * 255).astype(np.uint8).copy()

            # Draw ground truth boxes (Green)
            gt_count = 0
            if len(target['boxes']) > 0:
                for box in target['boxes']:
                    x1, y1, x2, y2 = box.cpu().numpy().astype(int)
                    cv2.rectangle(img_np, (x1, y1), (x2, y2), colors['ground_truth'], 3)
                    cv2.putText(img_np, 'GT', (x1, y1-8), cv2.FONT_HERSHEY_SIMPLEX,
                               0.6, colors['ground_truth'], 2)
                    gt_count += 1

            # Draw predictions (Red)
            pred_count = 0
            if len(prediction['boxes']) > 0:
                for box, score in zip(prediction['boxes'], prediction['scores']):
                    if score >= confidence_threshold:
                        x1, y1, x2, y2 = box.cpu().numpy().astype(int)
                        cv2.rectangle(img_np, (x1, y1), (x2, y2), colors['prediction'], 3)
                        cv2.putText(img_np, f'{score:.2f}', (x1, y2+20),
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, colors['prediction'], 2)
                        pred_count += 1

            ax.imshow(img_np)
            ax.set_title(f'Sample {img_idx}\nGround Truth: {gt_count} ships\nPredictions: {pred_count} ships',
                        fontsize=11, fontweight='bold')
            ax.axis('off')

    plt.suptitle(f'Ship Detection Results (Confidence > {confidence_threshold})\n' +
                 f'Green: Ground Truth  Red: Model Predictions',
                 fontsize=16, fontweight='bold', y=0.98)
    plt.tight_layout()
    plt.show()

def evaluate_detection_performance(model, dataset, confidence_threshold=0.5, num_samples=50):
    """Evaluate model performance"""
    model.eval()

    results = {
        'total_images': 0,
        'total_gt_ships': 0,
        'total_predicted_ships': 0,
        'correct_count_predictions': 0,
        'confidence_scores': []
    }

    print("EVALUATING SHIP DETECTION PERFORMANCE")
    print("=" * 50)

    with torch.no_grad():
        num_to_evaluate = min(num_samples, len(dataset))

        for i in range(num_to_evaluate):
            image, target = dataset[i]
            image_tensor = image.unsqueeze(0).to(device)
            prediction = model(image_tensor)[0]

            # Count ground truth ships
            gt_count = len(target['boxes'])
            results['total_gt_ships'] += gt_count

            # Count predictions above threshold
            high_conf_predictions = prediction['scores'] >= confidence_threshold
            pred_count = high_conf_predictions.sum().item()
            results['total_predicted_ships'] += pred_count

            # Store confidence scores
            if len(prediction['scores']) > 0:
                results['confidence_scores'].extend(prediction['scores'].cpu().numpy())

            # Count "correct" predictions (same number as ground truth)
            if pred_count == gt_count:
                results['correct_count_predictions'] += 1

            results['total_images'] += 1

    # Calculate metrics
    count_accuracy = (results['correct_count_predictions'] / results['total_images']) * 100
    avg_gt_per_image = results['total_gt_ships'] / results['total_images']
    avg_pred_per_image = results['total_predicted_ships'] / results['total_images']
    avg_confidence = np.mean(results['confidence_scores']) if results['confidence_scores'] else 0

    # Print results
    print(f"Evaluation Results on {results['total_images']} images:")
    print(f"   Count Accuracy: {count_accuracy:.1f}%")
    print(f"   Avg Ground Truth Ships/Image: {avg_gt_per_image:.2f}")
    print(f"   Avg Predicted Ships/Image: {avg_pred_per_image:.2f}")
    print(f"   Average Confidence Score: {avg_confidence:.3f}")
    print(f"   Confidence Threshold: {confidence_threshold}")

    return results

def show_training_summary():
    """Show training summary if available"""
    if 'dataset_info' in globals():
        info = dataset_info
        print("TRAINING SUMMARY")
        print("=" * 30)
        print(f"Dataset Size: {info['total_samples']} images")
        print(f"Training Set: {info['train_samples']} images")
        print(f"Validation Set: {info['val_samples']} images")
        print(f"Training Time: {info['training_time']:.1f} seconds")
        print(f"Final Training Loss: {info['final_loss']:.4f}")
        print(f"Device Used: {device}")

# Main visualization execution
print("SHIP DETECTION VISUALIZATION & EVALUATION")
print("=" * 60)

# Check if training completed successfully
if 'trained_model' in globals() and 'val_dataset' in globals():

    # Show training summary
    show_training_summary()

    print(f"\nVISUALIZING DETECTION RESULTS...")
    # Visualize predictions
    visualize_ship_detection(trained_model, val_dataset, num_images=6, confidence_threshold=0.5)

    print(f"\nPERFORMANCE EVALUATION...")
    # Evaluate performance
    eval_results = evaluate_detection_performance(
        trained_model, val_dataset,
        confidence_threshold=0.5,
        num_samples=50
    )

    # Create confidence score histogram
    if eval_results['confidence_scores']:
        plt.figure(figsize=(10, 6))
        plt.hist(eval_results['confidence_scores'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')
        plt.axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='Threshold (0.5)')
        plt.title('Distribution of Prediction Confidence Scores', fontsize=14, fontweight='bold')
        plt.xlabel('Confidence Score')
        plt.ylabel('Frequency')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()

    print(f"\nEVALUATION COMPLETED!")
    print(f"Your Faster R-CNN model is ready for ship detection!")

else:
    print("Model or validation dataset not found!")
    print("Please run the complete training pipeline first.")

"""
Faster R-CNN Ship Detection Performance Evaluation (Standalone)
Loads saved model and calculates comprehensive performance metrics
"""

import os
import xml.etree.ElementTree as ET
import torch
import torchvision
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from torchvision.transforms import functional as F
from torch.utils.data import Dataset
from torchvision.ops import box_iou
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import time

# Set style
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

print("FASTER R-CNN STANDALONE PERFORMANCE EVALUATION")
print("=" * 60)

# Configuration
MODEL_PATH = "/content/drive/MyDrive/faster_rcnn_ship_detector.pth"
DATASET_PATH = "/content/ship_dataset_extracted"  # Will search here first, then Drive
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# If dataset not extracted, extract it
import zipfile

ZIP_PATH = "/content/drive/MyDrive/Optical_yolo.zip"
if not os.path.exists(DATASET_PATH) and os.path.exists(ZIP_PATH):
    print(f"Extracting dataset from {ZIP_PATH}...")
    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:
        zip_ref.extractall(DATASET_PATH)
    print(f"Dataset extracted to {DATASET_PATH}")

# Dataset class
def parse_xml_annotation(xml_path):
    """Parse XML annotation and return bounding boxes and labels"""
    try:
        tree = ET.parse(xml_path)
        root = tree.getroot()
        boxes = []
        labels = []

        for obj in root.findall('object'):
            bbox = obj.find('bndbox')
            xmin = float(bbox.find('xmin').text)
            ymin = float(bbox.find('ymin').text)
            xmax = float(bbox.find('xmax').text)
            ymax = float(bbox.find('ymax').text)

            if xmax > xmin and ymax > ymin:
                boxes.append([xmin, ymin, xmax, ymax])
                labels.append(1)

        return boxes, labels
    except:
        return [], []

class OpticalShipDataset(Dataset):
    """Dataset for optical ship images"""

    def __init__(self, images_dir, annotations_dir):
        self.images_dir = images_dir
        self.annotations_dir = annotations_dir
        self.image_files = []

        for f in os.listdir(images_dir):
            if f.lower().endswith(('.jpg', '.jpeg', '.png')):
                base_name = os.path.splitext(f)[0]
                xml_path = os.path.join(annotations_dir, base_name + '.xml')
                if os.path.exists(xml_path):
                    self.image_files.append(f)

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        img_name = self.image_files[idx]
        img_path = os.path.join(self.images_dir, img_name)

        image = Image.open(img_path).convert('RGB')
        width, height = image.size

        base_name = os.path.splitext(img_name)[0]
        xml_path = os.path.join(self.annotations_dir, base_name + '.xml')
        boxes, labels = parse_xml_annotation(xml_path)

        if len(boxes) == 0:
            boxes = torch.zeros((0, 4), dtype=torch.float32)
            labels = torch.zeros((0,), dtype=torch.int64)
        else:
            boxes = np.array(boxes)
            boxes[:, 0] = np.clip(boxes[:, 0], 0, width)
            boxes[:, 1] = np.clip(boxes[:, 1], 0, height)
            boxes[:, 2] = np.clip(boxes[:, 2], 0, width)
            boxes[:, 3] = np.clip(boxes[:, 3], 0, height)
            boxes = torch.as_tensor(boxes, dtype=torch.float32)
            labels = torch.as_tensor(labels, dtype=torch.int64)

        image = F.to_tensor(image)

        target = {
            'boxes': boxes,
            'labels': labels,
            'image_id': torch.tensor([idx], dtype=torch.int64),
        }

        return image, target

def find_dataset():
    """Find dataset folders - updated for actual structure"""
    print("\nSearching for dataset...")

    # Search in multiple possible locations based on your structure
    search_paths = [
        DATASET_PATH,
        "/content/drive/MyDrive/Opitical_yolo",
        "/content/drive/MyDrive/Optical_yolo",
    ]

    for search_root in search_paths:
        if not os.path.exists(search_root):
            continue

        print(f"   Checking: {search_root}")

        for root, dirs, files in os.walk(search_root):
            if '__MACOSX' in root:
                continue

            # Look for JPEGImages and annotations folders
            if 'JPEGImages' in dirs and 'annotations' in dirs:
                images_dir = os.path.join(root, 'JPEGImages')
                annotations_dir = os.path.join(root, 'annotations')

                # Check if they have files
                img_files = [f for f in os.listdir(images_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
                xml_files = [f for f in os.listdir(annotations_dir) if f.lower().endswith('.xml')]

                if len(img_files) > 0 and len(xml_files) > 0:
                    print(f"Found dataset:")
                    print(f"   Images: {images_dir} ({len(img_files)} files)")
                    print(f"   Annotations: {annotations_dir} ({len(xml_files)} files)")
                    return images_dir, annotations_dir

    print("No valid dataset found!")
    print("\nAvailable directories in Drive:")
    if os.path.exists("/content/drive/MyDrive"):
        for item in os.listdir("/content/drive/MyDrive")[:10]:
            print(f"   - {item}")

    return None, None

def create_model(num_classes=2):
    """Create Faster R-CNN model"""
    model = fasterrcnn_resnet50_fpn(pretrained=False)
    in_features = model.roi_heads.box_predictor.cls_score.in_features
    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(
        in_features, num_classes
    )
    return model

def calculate_metrics(predictions, ground_truths, iou_threshold=0.5):
    """Calculate precision, recall, and mAP"""
    true_positives = 0
    false_positives = 0
    false_negatives = 0

    for pred, gt in zip(predictions, ground_truths):
        pred_boxes = pred['boxes']
        gt_boxes = gt['boxes']

        if len(gt_boxes) == 0:
            if len(pred_boxes) > 0:
                false_positives += len(pred_boxes)
            continue

        if len(pred_boxes) == 0:
            false_negatives += len(gt_boxes)
            continue

        iou_matrix = box_iou(pred_boxes, gt_boxes)
        matched_gt = set()

        for pred_idx in range(len(pred_boxes)):
            max_iou = 0
            max_gt_idx = -1

            for gt_idx in range(len(gt_boxes)):
                if gt_idx not in matched_gt:
                    iou = iou_matrix[pred_idx, gt_idx].item()
                    if iou > max_iou:
                        max_iou = iou
                        max_gt_idx = gt_idx

            if max_iou >= iou_threshold:
                true_positives += 1
                matched_gt.add(max_gt_idx)
            else:
                false_positives += 1

        false_negatives += len(gt_boxes) - len(matched_gt)

    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0
    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

    return {
        'precision': precision,
        'recall': recall,
        'f1_score': f1_score,
        'true_positives': true_positives,
        'false_positives': false_positives,
        'false_negatives': false_negatives
    }

def evaluate_model(model, dataset, device, conf_threshold=0.5):
    """Evaluate model on dataset"""
    print("\nRunning model evaluation...")

    model.eval()
    predictions = []
    ground_truths = []
    inference_times = []

    with torch.no_grad():
        for idx in range(len(dataset)):
            img, target = dataset[idx]
            img = img.to(device)

            start_time = time.time()
            pred = model([img])[0]
            inference_time = time.time() - start_time
            inference_times.append(inference_time)

            keep = pred['scores'] >= conf_threshold
            filtered_pred = {
                'boxes': pred['boxes'][keep].cpu(),
                'scores': pred['scores'][keep].cpu(),
                'labels': pred['labels'][keep].cpu()
            }

            predictions.append(filtered_pred)
            ground_truths.append({
                'boxes': target['boxes'].cpu(),
                'labels': target['labels'].cpu()
            })

            if (idx + 1) % 50 == 0:
                print(f"   Processed {idx + 1}/{len(dataset)} images...")

    print("Evaluation complete!")

    # Calculate metrics
    print("\nCalculating performance metrics...")

    metrics_50 = calculate_metrics(predictions, ground_truths, iou_threshold=0.5)
    metrics_75 = calculate_metrics(predictions, ground_truths, iou_threshold=0.75)

    map_scores = []
    for iou_thresh in np.arange(0.5, 1.0, 0.05):
        m = calculate_metrics(predictions, ground_truths, iou_threshold=iou_thresh)
        map_scores.append(m['precision'])

    map_50_95 = np.mean(map_scores)

    total_predictions = sum(len(p['boxes']) for p in predictions)
    total_ground_truth = sum(len(gt['boxes']) for gt in ground_truths)
    images_with_detections = sum(1 for p in predictions if len(p['boxes']) > 0)
    avg_inference_time = np.mean(inference_times)

    return {
        'mAP50': metrics_50['precision'],
        'mAP50_95': map_50_95,
        'Precision': metrics_50['precision'],
        'Recall': metrics_50['recall'],
        'F1_Score': metrics_50['f1_score'],
        'Precision@75': metrics_75['precision'],
        'Recall@75': metrics_75['recall'],
        'total_predictions': total_predictions,
        'total_ground_truth': total_ground_truth,
        'images_with_detections': images_with_detections,
        'avg_inference_time': avg_inference_time,
        'predictions': predictions,
        'ground_truths': ground_truths
    }

# Main execution
print(f"\nLoading model from: {MODEL_PATH}")

if not os.path.exists(MODEL_PATH):
    print(f"Model not found at {MODEL_PATH}")
    print("Please ensure you've trained the model first")
    exit()

# Load model
model = create_model(num_classes=2)
model.load_state_dict(torch.load(MODEL_PATH, map_location=device))
model.to(device)
model.eval()
print(f"Model loaded successfully on {device}")

# Find and load dataset
images_dir, annotations_dir = find_dataset()

if not images_dir:
    print("Dataset not found!")
    print(f"Please ensure dataset is extracted at: {DATASET_PATH}")
    exit()

# Create dataset (use full dataset as validation for this evaluation)
dataset = OpticalShipDataset(images_dir, annotations_dir)
print(f"Loaded dataset with {len(dataset)} images")

# Run evaluation
results = evaluate_model(model, dataset, device, conf_threshold=0.5)

# Print results
print("\n" + "=" * 60)
print("PERFORMANCE METRICS")
print("=" * 60)
print(f"mAP@0.5:        {results['mAP50']:.4f}")
print(f"mAP@0.5:0.95:   {results['mAP50_95']:.4f}")
print(f"Precision:      {results['Precision']:.4f}")
print(f"Recall:         {results['Recall']:.4f}")
print(f"F1-Score:       {results['F1_Score']:.4f}")
print(f"Precision@0.75: {results['Precision@75']:.4f}")
print(f"Recall@0.75:    {results['Recall@75']:.4f}")
print(f"\nDetection Statistics:")
print(f"Total predictions:  {results['total_predictions']}")
print(f"Total ground truth: {results['total_ground_truth']}")
print(f"Images with ships:  {results['images_with_detections']}/{len(dataset)}")
print(f"Avg inference time: {results['avg_inference_time']*1000:.2f} ms")

# Create visualizations
metrics = {
    'mAP50': results['mAP50'],
    'mAP50_95': results['mAP50_95'],
    'Precision': results['Precision'],
    'Recall': results['Recall'],
    'F1_Score': results['F1_Score']
}

fig = plt.figure(figsize=(20, 12))

# 1. Performance Metrics
ax1 = plt.subplot(2, 3, 1)
metric_names = list(metrics.keys())
metric_values = list(metrics.values())
colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12', '#9b59b6']
bars = ax1.bar(metric_names, metric_values, color=colors, alpha=0.8, edgecolor='black', linewidth=2)

for bar, value in zip(bars, metric_values):
    height = bar.get_height()
    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.02,
             f'{value:.4f}', ha='center', va='bottom', fontweight='bold', fontsize=10)

ax1.set_ylabel('Score', fontsize=12, fontweight='bold')
ax1.set_title('Faster R-CNN Performance Metrics', fontsize=14, fontweight='bold', pad=20)
ax1.set_ylim(0, 1.0)
ax1.grid(axis='y', alpha=0.3)
plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right')

# 2. Precision-Recall
ax2 = plt.subplot(2, 3, 2)
ax2.scatter([results['Recall']], [results['Precision']],
           s=300, c='red', marker='*', edgecolors='black',
           linewidths=2, zorder=5, label='Model Performance')
ax2.plot([0, 1], [0, 1], 'k--', alpha=0.3, label='Random Classifier')
ax2.set_xlabel('Recall', fontsize=12, fontweight='bold')
ax2.set_ylabel('Precision', fontsize=12, fontweight='bold')
ax2.set_title('Precision-Recall Trade-off', fontsize=14, fontweight='bold', pad=20)
ax2.set_xlim(0, 1)
ax2.set_ylim(0, 1)
ax2.legend(loc='lower left', fontsize=10)
ax2.grid(True, alpha=0.3)
ax2.annotate(f'P={results["Precision"]:.3f}\nR={results["Recall"]:.3f}\nF1={results["F1_Score"]:.3f}',
             xy=(results['Recall'], results['Precision']),
             xytext=(0.6, 0.3),
             fontsize=11, fontweight='bold',
             bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7),
             arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0.3', lw=2))

# 3. IoU Analysis
ax3 = plt.subplot(2, 3, 3)
iou_thresholds = [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]
precisions = []
recalls = []

for thresh in iou_thresholds:
    m = calculate_metrics(results['predictions'], results['ground_truths'], iou_threshold=thresh)
    precisions.append(m['precision'])
    recalls.append(m['recall'])

ax3.plot(iou_thresholds, precisions, 'b-o', linewidth=2, markersize=6, label='Precision')
ax3.plot(iou_thresholds, recalls, 'r-s', linewidth=2, markersize=6, label='Recall')
ax3.set_xlabel('IoU Threshold', fontsize=12, fontweight='bold')
ax3.set_ylabel('Score', fontsize=12, fontweight='bold')
ax3.set_title('Performance vs IoU Threshold', fontsize=14, fontweight='bold', pad=20)
ax3.legend(loc='upper right', fontsize=10)
ax3.grid(True, alpha=0.3)

# 4. Radar Chart
ax4 = plt.subplot(2, 3, 4, projection='polar')
categories = ['mAP50', 'mAP50-95', 'Precision', 'Recall', 'F1-Score']
values = list(metrics.values())
values += values[:1]

angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
angles += angles[:1]

ax4.plot(angles, values, 'o-', linewidth=2, color='#2ecc71', label='Faster R-CNN')
ax4.fill(angles, values, alpha=0.25, color='#2ecc71')
ax4.set_xticks(angles[:-1])
ax4.set_xticklabels(categories, fontsize=10)
ax4.set_ylim(0, 1)
ax4.set_title('Performance Radar Chart', fontsize=14, fontweight='bold', pad=20)
ax4.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))
ax4.grid(True)

# 5. Statistics
ax5 = plt.subplot(2, 3, 5)
stats_data = [len(dataset), results['total_predictions'],
              results['total_ground_truth'], results['images_with_detections']]
stats_labels = ['Total\nImages', 'Ships\nDetected', 'Ground\nTruth', 'Images with\nDetections']
colors_stats = ['#3498db', '#e74c3c', '#2ecc71', '#9b59b6']

bars = ax5.bar(stats_labels, stats_data, color=colors_stats, alpha=0.8, edgecolor='black', linewidth=2)
for bar, value in zip(bars, stats_data):
    height = bar.get_height()
    ax5.text(bar.get_x() + bar.get_width()/2., height + max(stats_data)*0.02,
             str(int(value)), ha='center', va='bottom', fontweight='bold', fontsize=10)

ax5.set_ylabel('Count', fontsize=12, fontweight='bold')
ax5.set_title('Dataset Statistics', fontsize=14, fontweight='bold', pad=20)
ax5.grid(axis='y', alpha=0.3)

# 6. Speed
ax6 = plt.subplot(2, 3, 6)
speed_data = [results['avg_inference_time']*1000, 1/results['avg_inference_time']]
speed_labels = ['Inference\nTime (ms)', 'FPS']
colors_speed = ['#e74c3c', '#2ecc71']

bars = ax6.bar(speed_labels, speed_data, color=colors_speed, alpha=0.8, edgecolor='black', linewidth=2)
for bar, value in zip(bars, speed_data):
    height = bar.get_height()
    ax6.text(bar.get_x() + bar.get_width()/2., height + max(speed_data)*0.02,
             f'{value:.2f}', ha='center', va='bottom', fontweight='bold', fontsize=11)

ax6.set_ylabel('Value', fontsize=12, fontweight='bold')
ax6.set_title(f'Inference Speed ({device})', fontsize=14, fontweight='bold', pad=20)
ax6.grid(axis='y', alpha=0.3)

fig.suptitle('Faster R-CNN Ship Detection - Performance Analysis\n(ResNet-50 FPN with Transfer Learning)',
             fontsize=18, fontweight='bold', y=0.98)

plt.tight_layout(rect=[0, 0.03, 1, 0.96])

output_path = '/content/drive/MyDrive/faster_rcnn_performance_charts.png'
plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')
print(f"\nCharts saved: {output_path}")

plt.show()

print("\n" + "=" * 60)
print("EVALUATION COMPLETE!")
print("=" * 60)

"""
Faster R-CNN Optical Ship Detection - STS Transfer Detection
Detects close ships (potential Ship-to-Ship transfers) in optical imagery
"""

import os
import zipfile
import cv2
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import math
import random
from PIL import Image

import torch
import torchvision
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from torchvision.transforms import functional as F

from google.colab import drive

# Mount Drive with error handling
if not os.path.exists('/content/drive/MyDrive'):
    print("\nMounting Google Drive...")
    try:
        drive.mount('/content/drive')
    except:
        drive.mount('/content/drive', force_remount=True)
else:
    print("Google Drive already mounted")

print("\nFASTER R-CNN OPTICAL - STS TRANSFER DETECTION")
print("="*60)

# Configuration
STS_DISTANCE_THRESHOLD = 50  # pixels - ships closer than this flagged as STS
STS_MIN_CONFIDENCE = 0.3      # minimum confidence for STS detection
CONF_THRESHOLD = 0.25         # general detection threshold
NUM_EXAMPLES = 100            # number of images to analyze

# Paths
MODEL_PATH = "/content/drive/MyDrive/faster_rcnn_ship_detector.pth"
OPTICAL_ZIP = "/content/drive/MyDrive/Optical_yolo.zip"
EXTRACT_PATH = "/content/optical_extracted"

# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# Load Faster R-CNN Model
def create_model(num_classes=2):
    """Create Faster R-CNN model with same architecture as training"""
    model = fasterrcnn_resnet50_fpn(pretrained=False)

    # Replace the classifier head
    in_features = model.roi_heads.box_predictor.cls_score.in_features
    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(
        in_features, num_classes
    )

    return model

print(f"\nLoading Faster R-CNN model...")

if not os.path.exists(MODEL_PATH):
    print(f"\nModel not found at: {MODEL_PATH}")
    print("\nAvailable .pth files in MyDrive:")
    for file in os.listdir("/content/drive/MyDrive"):
        if file.endswith('.pth'):
            print(f"   - {file}")
    exit()

# Create model and load weights
model = create_model(num_classes=2)
model.load_state_dict(torch.load(MODEL_PATH, map_location=device))
model.to(device)
model.eval()

print(f"Model loaded from: {MODEL_PATH}")

# Extract optical dataset (always extract fresh in new notebook)
print(f"\nExtracting optical dataset...")

# Remove old extraction if exists
if os.path.exists(EXTRACT_PATH):
    import shutil
    shutil.rmtree(EXTRACT_PATH)
    print(f"   Removed old extraction")

# Check if ZIP exists
if not os.path.exists(OPTICAL_ZIP):
    print(f"Dataset ZIP not found at: {OPTICAL_ZIP}")
    print("\nLooking for ZIP files in MyDrive:")
    for file in os.listdir("/content/drive/MyDrive"):
        if file.endswith('.zip') and 'optical' in file.lower():
            print(f"   Found: {file}")
    exit()

# Extract
with zipfile.ZipFile(OPTICAL_ZIP, 'r') as zip_ref:
    zip_ref.extractall(EXTRACT_PATH)
print(f"Extracted to {EXTRACT_PATH}")

# Find test images
print("\nFinding test images...")
test_dirs = []

for root, dirs, files in os.walk(EXTRACT_PATH):
    # Skip macOS metadata folders
    if '__MACOSX' in root:
        continue

    if 'test' in root.lower() and ('JPEGImages' in dirs or any(f.lower().endswith(('.jpg', '.png')) for f in files)):
        if 'JPEGImages' in dirs:
            test_dirs.append(os.path.join(root, 'JPEGImages'))
        else:
            test_dirs.append(root)

if not test_dirs:
    # Fallback: find any image directory
    for root, dirs, files in os.walk(EXTRACT_PATH):
        if '__MACOSX' in root:
            continue
        img_files = [f for f in files if f.lower().endswith(('.jpg', '.png', '.jpeg')) and not f.startswith('._')]
        if len(img_files) > 10:
            test_dirs.append(root)
            break

if not test_dirs:
    print("No test images found")
    print(f"Extracted structure:")
    for root, dirs, files in os.walk(EXTRACT_PATH):
        print(f"   {root}")
    exit()

print(f"Found test directory: {test_dirs[0]}")

# Get test images
test_images = []
for test_dir in test_dirs:
    for ext in ['*.jpg', '*.jpeg', '*.png', '*.JPG']:
        test_images.extend(list(Path(test_dir).glob(ext)))

# Filter out hidden files
test_images = [img for img in test_images if not img.name.startswith('._')]

print(f"Found {len(test_images)} test images")

# STS Detection Functions
def calculate_distance(box1, box2):
    """Calculate distance between box centers"""
    center1 = ((box1[0] + box1[2]) / 2, (box1[1] + box1[3]) / 2)
    center2 = ((box2[0] + box2[2]) / 2, (box2[1] + box2[3]) / 2)
    return math.sqrt((center1[0] - center2[0])**2 + (center1[1] - center2[1])**2)

def detect_sts_transfers(boxes, confidences, threshold=50, min_conf=0.3):
    """Detect potential Ship-to-Ship transfers"""
    sts_pairs = []

    if len(boxes) < 2:
        return sts_pairs

    for i in range(len(boxes)):
        for j in range(i + 1, len(boxes)):
            if confidences[i] >= min_conf and confidences[j] >= min_conf:
                distance = calculate_distance(boxes[i], boxes[j])

                if distance <= threshold:
                    sts_pairs.append({
                        'ship1': i,
                        'ship2': j,
                        'distance': distance,
                        'confidence1': confidences[i],
                        'confidence2': confidences[j]
                    })

    return sts_pairs

def predict_image(image_path, model, device, conf_threshold=0.25):
    """Run Faster R-CNN prediction on an image"""
    # Load image
    image = Image.open(image_path).convert('RGB')
    image_tensor = F.to_tensor(image).unsqueeze(0).to(device)

    # Predict
    with torch.no_grad():
        predictions = model(image_tensor)

    # Extract boxes and scores for ships (class 1)
    boxes = []
    confidences = []

    pred = predictions[0]
    for box, label, score in zip(pred['boxes'], pred['labels'], pred['scores']):
        if label == 1 and score >= conf_threshold:  # Ship class and confidence threshold
            boxes.append(box.cpu().numpy())
            confidences.append(score.cpu().numpy())

    return boxes, confidences

# Run STS Detection
print(f"\nAnalyzing {NUM_EXAMPLES} images for STS transfers...")
print(f"   Distance threshold: {STS_DISTANCE_THRESHOLD}px")
print(f"   Confidence threshold: {STS_MIN_CONFIDENCE}")

# Select random sample
sample_images = random.sample(test_images, min(NUM_EXAMPLES*2, len(test_images)))

sts_results = []
regular_results = []

print("\nProcessing images...")
for idx, img_path in enumerate(sample_images):
    if idx % 10 == 0:
        print(f"   Processed {idx}/{len(sample_images)} images...")

    # Run detection
    boxes, confidences = predict_image(img_path, model, device, CONF_THRESHOLD)

    if len(boxes) >= 2:
        # Detect STS
        sts_pairs = detect_sts_transfers(boxes, confidences, STS_DISTANCE_THRESHOLD, STS_MIN_CONFIDENCE)

        result_data = {
            'image_path': img_path,
            'image_name': img_path.name,
            'boxes': boxes,
            'confidences': confidences,
            'sts_pairs': sts_pairs,
            'ship_count': len(boxes)
        }

        if len(sts_pairs) > 0:
            sts_results.append(result_data)
        else:
            regular_results.append(result_data)

    if len(sts_results) >= NUM_EXAMPLES:
        break

print(f"\nSTS Detection Results:")
print(f"   Images with STS transfers: {len(sts_results)}")
print(f"   Total STS pairs detected: {sum(len(r['sts_pairs']) for r in sts_results)}")
print(f"   Regular ship detections: {len(regular_results)}")

# Visualize STS Detections
if len(sts_results) > 0:
    print(f"\nCreating STS detection visualization...")

    # Show top examples
    top_sts = sorted(sts_results, key=lambda x: len(x['sts_pairs']), reverse=True)[:NUM_EXAMPLES]

    fig, axes = plt.subplots(10, 10, figsize=(50, 50))
    axes = axes.flatten()

    for idx, result in enumerate(top_sts[:100]):
        # Load image
        img = cv2.imread(str(result['image_path']))
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        # Get STS ship indices
        sts_indices = set()
        for pair in result['sts_pairs']:
            sts_indices.add(pair['ship1'])
            sts_indices.add(pair['ship2'])

        # Draw bounding boxes
        for i, (box, conf) in enumerate(zip(result['boxes'], result['confidences'])):
            x1, y1, x2, y2 = map(int, box)

            # Color: Red for STS ships, Green for regular
            if i in sts_indices:
                color = (255, 0, 0)  # Red
                thickness = 3
            else:
                color = (0, 255, 0)  # Green
                thickness = 2

            cv2.rectangle(img_rgb, (x1, y1), (x2, y2), color, thickness)

            # Confidence label
            label = f'{conf:.2f}'
            cv2.putText(img_rgb, label, (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX,
                       0.5, (255, 255, 255), 2)

        # Draw STS connection lines
        for pair in result['sts_pairs']:
            box1 = result['boxes'][pair['ship1']]
            box2 = result['boxes'][pair['ship2']]

            center1 = (int((box1[0] + box1[2])/2), int((box1[1] + box1[3])/2))
            center2 = (int((box2[0] + box2[2])/2), int((box2[1] + box2[3])/2))

            # Red line between STS ships
            cv2.line(img_rgb, center1, center2, (255, 0, 0), 2)

            # Distance annotation
            mid_x = int((center1[0] + center2[0])/2)
            mid_y = int((center1[1] + center2[1])/2)
            dist_text = f"{pair['distance']:.0f}px"
            cv2.putText(img_rgb, dist_text, (mid_x-20, mid_y),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)

        axes[idx].imshow(img_rgb)
        axes[idx].set_title(f"{result['image_name'][:20]}\n{result['ship_count']} ships, {len(result['sts_pairs'])} STS",
                           fontsize=10, color='red')
        axes[idx].axis('off')

    # Hide unused subplots
    for idx in range(len(top_sts), 100):
        axes[idx].axis('off')

    plt.suptitle(f'Faster R-CNN Optical - STS Transfer Detection (Threshold: {STS_DISTANCE_THRESHOLD}px)\nRed: STS Ships, Green: Regular Ships, Red Lines: Transfer Connections',
                 fontsize=16, fontweight='bold')
    plt.tight_layout()

    output_path = '/content/drive/MyDrive/faster_rcnn_optical_sts_detection.png'
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.show()

    print(f"Visualization saved: {output_path}")

    # Summary statistics
    print(f"\nSTS Detection Summary:")
    print(f"   Total images analyzed: {len(sample_images)}")
    print(f"   Images with STS: {len(sts_results)}")
    print(f"   Total STS pairs: {sum(len(r['sts_pairs']) for r in sts_results)}")
    if sts_results:
        print(f"   Average distance: {np.mean([p['distance'] for r in sts_results for p in r['sts_pairs']]):.1f}px")

    # Top STS examples
    print(f"\nTop STS Examples:")
    for i, result in enumerate(top_sts[:5], 1):
        print(f"   {i}. {result['image_name'][:30]} - {len(result['sts_pairs'])} transfers, {result['ship_count']} ships total")

else:
    print(f"\nNo STS transfers detected in sampled images")
    print(f"   Try adjusting the distance threshold (currently {STS_DISTANCE_THRESHOLD}px)")

print(f"\nSTS DETECTION COMPLETE!")
print(f"   Model: Faster R-CNN")
print(f"   Threshold: {STS_DISTANCE_THRESHOLD}px")
print(f"   Min confidence: {STS_MIN_CONFIDENCE}")
print(f"   Results saved to Google Drive")