# -*- coding: utf-8 -*-
"""Copy of fast r cnn SAR DONE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OMnQ-iD_tNpVnB_jp402_MxAKtqXHYF9
"""

# @title
# Complete Faster R-CNN SAR Ship Detection with Fixed STS Detection
# Includes installation, training, and improved STS analysis

# ==================== SETUP AND INSTALLATIONS ====================
import os
import shutil
from pathlib import Path
import math
import random
import zipfile
import json
from collections import defaultdict

# Install required packages (run when reconnected to runtime)
!pip install torch torchvision
!pip install pycocotools
!pip install opencv-python
!pip install matplotlib
!pip install albumentations
!pip install scikit-learn

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Import libraries
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torchvision
from torchvision import transforms
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import cv2
import numpy as np
import matplotlib.pyplot as plt
import albumentations as A
from albumentations.pytorch import ToTensorV2
from sklearn.model_selection import train_test_split
from PIL import Image
import warnings
warnings.filterwarnings('ignore')

print(f"PyTorch version: {torch.__version__}")
print(f"Torchvision version: {torchvision.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")

# ==================== CONFIGURATION ====================

# Dataset Configuration
ZIP_FILE_PATH = "/content/drive/MyDrive/SAR_Dataset.zip"  # Your dataset zip path
DATASET_PATH = "/content/sar_dataset"
EXTRACTED_PATH = "/content/extracted_dataset"

# Training Configuration - ADJUSTED FOR OVERFITTING PREVENTION
EPOCHS = 30  # Increased epochs but with better regularization
BATCH_SIZE = 8  # Your optimized batch size for A100
LEARNING_RATE = 0.0003  # Reduced learning rate
WEIGHT_DECAY = 0.001  # Increased weight decay for regularization
VALIDATION_SPLIT = 0.25  # Increased validation split
PATIENCE = 10  # Increased patience
DROPOUT_RATE = 0.3  # Add dropout for regularization

# Image Configuration
IMG_SIZE = 640  # Input image size
CONFIDENCE_THRESHOLD = 0.4  # Lowered confidence threshold

# STS Detection Configuration
STS_DISTANCE_THRESHOLD = 60  # Increased threshold for SAR images
STS_MIN_CONFIDENCE = 0.3  # Lowered for more detections

# Output directories
OUTPUT_DIR = "/content/drive/MyDrive/FasterRCNN_SAR_Results"
MODEL_SAVE_PATH = "/content/drive/MyDrive/faster_rcnn_sar_best.pth"
METRICS_SAVE_PATH = "/content/drive/MyDrive/training_metrics.json"
STS_OUTPUT_DIR = "/content/drive/MyDrive/STS_Detection_Results"

os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(STS_OUTPUT_DIR, exist_ok=True)

print("Improved Faster R-CNN Configuration:")
print(f"- Epochs: {EPOCHS}")
print(f"- Batch size: {BATCH_SIZE}")
print(f"- Learning rate: {LEARNING_RATE} (reduced)")
print(f"- Weight decay: {WEIGHT_DECAY} (increased)")
print(f"- Validation split: {VALIDATION_SPLIT} (increased)")
print(f"- Dropout rate: {DROPOUT_RATE} (added)")

# ==================== DATASET EXTRACTION ====================

def extract_and_organize_dataset():
    """Extract zip file and organize images and labels properly"""

    print("Extracting dataset from zip file...")

    # Extract zip file
    with zipfile.ZipFile(ZIP_FILE_PATH, 'r') as zip_ref:
        zip_ref.extractall(EXTRACTED_PATH)

    print(f"Dataset extracted to: {EXTRACTED_PATH}")

    # Create target directories
    images_dir = f"{EXTRACTED_PATH}/images"
    labels_dir = f"{EXTRACTED_PATH}/labels"
    os.makedirs(images_dir, exist_ok=True)
    os.makedirs(labels_dir, exist_ok=True)

    # Find all files recursively
    all_files = []
    for root, dirs, files in os.walk(EXTRACTED_PATH):
        for file in files:
            if not file.startswith('._') and not file.startswith('.DS_Store'):
                all_files.append(os.path.join(root, file))

    print(f"Found {len(all_files)} files after extraction")

    # Separate and organize files
    image_extensions = ['.jpg', '.jpeg', '.png', '.tif', '.tiff', '.bmp']
    label_extensions = ['.txt']

    image_count = 0
    label_count = 0

    for file_path in all_files:
        file_name = os.path.basename(file_path)
        file_ext = os.path.splitext(file_name)[1].lower()

        if file_ext in image_extensions:
            target_path = os.path.join(images_dir, file_name)
            shutil.copy2(file_path, target_path)
            image_count += 1
        elif file_ext in label_extensions:
            target_path = os.path.join(labels_dir, file_name)
            shutil.copy2(file_path, target_path)
            label_count += 1

    print(f"Organized {image_count} images and {label_count} label files")
    return images_dir, labels_dir, image_count, label_count

# ==================== IMPROVED DATASET CLASS ====================

class SARShipDataset(Dataset):
    """Improved dataset class with better augmentation for overfitting prevention"""

    def __init__(self, images_dir, labels_dir, image_files, transforms=None, mode='train'):
        self.images_dir = images_dir
        self.labels_dir = labels_dir
        self.image_files = image_files
        self.transforms = transforms
        self.mode = mode

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        # Load image
        img_name = self.image_files[idx]
        img_path = os.path.join(self.images_dir, img_name)

        # Read image as grayscale and convert to RGB
        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
        if image is None:
            image = np.zeros((IMG_SIZE, IMG_SIZE), dtype=np.uint8)

        # Convert grayscale to RGB (3 channels)
        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)

        # Load corresponding label
        label_name = os.path.splitext(img_name)[0] + '.txt'
        label_path = os.path.join(self.labels_dir, label_name)

        boxes = []
        labels = []

        if os.path.exists(label_path):
            with open(label_path, 'r') as f:
                lines = f.readlines()

            for line in lines:
                line = line.strip()
                if line:
                    parts = line.split()
                    if len(parts) >= 5:
                        # YOLO format: class_id x_center y_center width height (normalized)
                        class_id = int(parts[0])
                        x_center = float(parts[1])
                        y_center = float(parts[2])
                        width = float(parts[3])
                        height = float(parts[4])

                        # Convert to absolute coordinates
                        img_h, img_w = image.shape[:2]
                        x_center *= img_w
                        y_center *= img_h
                        width *= img_w
                        height *= img_h

                        # Convert to corner coordinates (x1, y1, x2, y2)
                        x1 = x_center - width / 2
                        y1 = y_center - height / 2
                        x2 = x_center + width / 2
                        y2 = y_center + height / 2

                        # Ensure coordinates are within image bounds
                        x1 = max(0, min(x1, img_w-1))
                        y1 = max(0, min(y1, img_h-1))
                        x2 = max(x1+1, min(x2, img_w))
                        y2 = max(y1+1, min(y2, img_h))

                        boxes.append([x1, y1, x2, y2])
                        labels.append(1)  # Ship class = 1

        # Handle empty labels
        if len(boxes) == 0:
            boxes = [[0, 0, 1, 1]]  # Dummy box
            labels = [0]  # Background

        # Convert to tensors
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        labels = torch.as_tensor(labels, dtype=torch.int64)

        # Create target dictionary
        target = {
            'boxes': boxes,
            'labels': labels,
            'image_id': torch.tensor([idx])
        }

        # Apply transforms
        if self.transforms:
            try:
                if self.mode == 'train':
                    # Apply augmentations during training
                    transformed = self.transforms(image=image, bboxes=boxes.numpy(), class_labels=labels.numpy())
                    image = transformed['image']  # This is already a tensor from ToTensorV2
                    if len(transformed['bboxes']) > 0:
                        target['boxes'] = torch.as_tensor(transformed['bboxes'], dtype=torch.float32)
                        target['labels'] = torch.as_tensor(transformed['class_labels'], dtype=torch.int64)
                else:
                    # Apply validation transforms
                    transformed = self.transforms(image=image, bboxes=boxes.numpy(), class_labels=labels.numpy())
                    image = transformed['image']  # This is already a tensor from ToTensorV2
                    if len(transformed['bboxes']) > 0:
                        target['boxes'] = torch.as_tensor(transformed['bboxes'], dtype=torch.float32)
                        target['labels'] = torch.as_tensor(transformed['class_labels'], dtype=torch.int64)
            except Exception as e:
                print(f"Transform error: {e}")
                # Fallback: manual transform
                image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0
        else:
            # Default transform
            image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0

        return image, target

# ==================== IMPROVED DATA AUGMENTATION ====================

def get_train_transforms():
    """Enhanced training transforms with stronger augmentation for overfitting prevention"""
    return A.Compose([
        A.Resize(IMG_SIZE, IMG_SIZE),
        A.HorizontalFlip(p=0.5),
        A.VerticalFlip(p=0.3),  # Added vertical flip
        A.RandomRotate90(p=0.4),
        A.Rotate(limit=15, p=0.3),  # Added rotation
        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.4),
        A.GaussNoise(var_limit=(10.0, 30.0), p=0.3),
        A.Blur(blur_limit=3, p=0.3),
        A.RandomGamma(gamma_limit=(80, 120), p=0.2),  # Added gamma correction
        A.CLAHE(p=0.2),  # Added CLAHE for SAR images
        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ToTensorV2(),
    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels'], min_visibility=0.3))

def get_val_transforms():
    """Validation transforms (no augmentation)"""
    return A.Compose([
        A.Resize(IMG_SIZE, IMG_SIZE),
        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ToTensorV2(),
    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))

# ==================== IMPROVED MODEL WITH REGULARIZATION ====================

def create_faster_rcnn_model(num_classes=2):
    """Create Faster R-CNN model with additional regularization"""

    # Load pre-trained model
    model = fasterrcnn_resnet50_fpn(pretrained=True)

    # Replace the classifier head
    in_features = model.roi_heads.box_predictor.cls_score.in_features
    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)

    # Add dropout to ROI heads for regularization
    if hasattr(model.roi_heads.box_predictor, 'cls_score'):
        model.roi_heads.box_predictor.cls_score = nn.Sequential(
            nn.Dropout(DROPOUT_RATE),
            model.roi_heads.box_predictor.cls_score
        )

    return model

# ==================== TRAINING FUNCTIONS ====================

def train_one_epoch(model, data_loader, optimizer, device, epoch):
    """Train for one epoch"""
    model.train()
    total_loss = 0
    num_batches = 0

    for batch_idx, (images, targets) in enumerate(data_loader):
        images = [img.to(device) for img in images]
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

        optimizer.zero_grad()
        loss_dict = model(images, targets)
        losses = sum(loss for loss in loss_dict.values())

        losses.backward()

        # Gradient clipping for stability
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        optimizer.step()

        total_loss += losses.item()
        num_batches += 1

        if batch_idx % 50 == 0:
            print(f"Epoch {epoch}, Batch {batch_idx}/{len(data_loader)}, Loss: {losses.item():.4f}")

    return total_loss / num_batches

def evaluate_model(model, data_loader, device):
    """Evaluate model on validation set"""
    model.train()  # Keep in training mode to get loss
    total_loss = 0
    num_batches = 0

    with torch.no_grad():
        for images, targets in data_loader:
            images = [img.to(device) for img in images]
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            loss_dict = model(images, targets)
            losses = sum(loss for loss in loss_dict.values())
            total_loss += losses.item()
            num_batches += 1

    model.eval()
    return total_loss / num_batches

# ==================== FIXED STS DETECTION FUNCTIONS ====================

def calculate_distance(box1, box2):
    """Calculate distance between centers of two bounding boxes"""
    center1 = ((box1[0] + box1[2]) / 2, (box1[1] + box1[3]) / 2)
    center2 = ((box2[0] + box2[2]) / 2, (box2[1] + box2[3]) / 2)
    distance = math.sqrt((center1[0] - center2[0])**2 + (center1[1] - center2[1])**2)
    return distance

def detect_sts_transfers(boxes, confidences):
    """Detect potential Ship-to-Ship (STS) transfers"""
    sts_pairs = []
    sts_boxes = []

    if len(boxes) < 2:
        return sts_pairs, sts_boxes

    for i in range(len(boxes)):
        for j in range(i + 1, len(boxes)):
            if confidences[i] >= STS_MIN_CONFIDENCE and confidences[j] >= STS_MIN_CONFIDENCE:
                distance = calculate_distance(boxes[i], boxes[j])

                if distance <= STS_DISTANCE_THRESHOLD:
                    sts_pairs.append({
                        'ship1': i,
                        'ship2': j,
                        'distance': distance,
                        'confidence1': confidences[i],
                        'confidence2': confidences[j]
                    })
                    sts_boxes.extend([i, j])

    return sts_pairs, list(set(sts_boxes))

def analyze_images_for_sts_fixed(model, dataset, device, num_images=50):
    """Fixed STS analysis using the same preprocessing as training"""

    print(f"Analyzing images for STS transfers with consistent preprocessing...")
    print(f"Distance threshold: {STS_DISTANCE_THRESHOLD} pixels")
    print(f"Confidence threshold: {STS_MIN_CONFIDENCE}")
    print("=" * 60)

    # Use validation dataset for consistency
    indices = random.sample(range(len(dataset)), min(num_images, len(dataset)))

    sts_results = []
    regular_results = []

    model.eval()

    for idx_pos, dataset_idx in enumerate(indices):
        try:
            # Get image and target using the same preprocessing as training
            image, target = dataset[dataset_idx]

            # Prepare for model (image is already preprocessed by dataset)
            img_tensor = image.unsqueeze(0).to(device)

            # Get predictions
            with torch.no_grad():
                predictions = model(img_tensor)[0]

            # Extract detections
            boxes = predictions['boxes'].cpu().numpy()
            scores = predictions['scores'].cpu().numpy()
            labels = predictions['labels'].cpu().numpy()

            # Filter by confidence and ship class
            valid_detections = []
            valid_scores = []

            for box, score, label in zip(boxes, scores, labels):
                if score >= STS_MIN_CONFIDENCE and label == 1:  # Ship class
                    valid_detections.append(box)
                    valid_scores.append(score)

            if len(valid_detections) >= 2:
                # Check for STS transfers
                sts_pairs, sts_box_indices = detect_sts_transfers(valid_detections, valid_scores)

                result = {
                    'dataset_idx': dataset_idx,
                    'image_name': dataset.image_files[dataset_idx],
                    'total_ships': len(valid_detections),
                    'sts_pairs': sts_pairs,
                    'sts_count': len(sts_pairs),
                    'boxes': valid_detections,
                    'scores': valid_scores,
                    'sts_box_indices': sts_box_indices,
                    'original_image': image  # Store preprocessed image
                }

                if len(sts_pairs) > 0:
                    sts_results.append(result)
                    print(f"STS FOUND: {result['image_name']} - {len(sts_pairs)} transfer(s), {len(valid_detections)} ships")
                else:
                    regular_results.append(result)

            if (idx_pos + 1) % 10 == 0:
                print(f"Processed {idx_pos + 1}/{len(indices)} images...")

        except Exception as e:
            print(f"Error processing image {dataset_idx}: {e}")
            continue

    print(f"\nSTS ANALYSIS COMPLETE:")
    print(f"Images with STS transfers: {len(sts_results)}")
    print(f"Images with regular ships: {len(regular_results)}")
    print(f"Total STS transfer pairs found: {sum(r['sts_count'] for r in sts_results)}")

    return sts_results, regular_results

def visualize_sts_results_fixed(sts_results, dataset, max_images=20):
    """Visualize STS results with consistent preprocessing"""

    if len(sts_results) == 0:
        print("No STS transfers found to visualize")
        return

    # Sort by number of STS transfers
    sts_results_sorted = sorted(sts_results, key=lambda x: x['sts_count'], reverse=True)
    display_results = sts_results_sorted[:max_images]

    # Calculate grid size
    num_images = len(display_results)
    cols = 4
    rows = math.ceil(num_images / cols)

    fig, axes = plt.subplots(rows, cols, figsize=(20, 5*rows))
    if rows == 1:
        axes = [axes] if cols == 1 else axes
    else:
        axes = axes.flatten()

    for idx, result in enumerate(display_results):
        # Load original image for visualization
        img_path = os.path.join(dataset.images_dir, result['image_name'])
        original_image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
        if original_image is not None:
            image_rgb = cv2.cvtColor(original_image, cv2.COLOR_GRAY2RGB)
            image_rgb = cv2.resize(image_rgb, (IMG_SIZE, IMG_SIZE))  # Resize to match model input
        else:
            # Fallback to processed image
            image_rgb = result['original_image'].permute(1, 2, 0).numpy()
            image_rgb = ((image_rgb * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])) * 255).astype(np.uint8)

        # Draw bounding boxes
        for box_idx, (box, score) in enumerate(zip(result['boxes'], result['scores'])):
            x1, y1, x2, y2 = box.astype(int)

            # Choose color: Red for STS ships, Green for regular ships
            if box_idx in result['sts_box_indices']:
                color = (255, 0, 0)  # Red for STS
                thickness = 4
            else:
                color = (0, 255, 0)  # Green for regular ships
                thickness = 2

            # Draw bounding box
            cv2.rectangle(image_rgb, (x1, y1), (x2, y2), color, thickness)

            # Add confidence label
            label = f'{score:.2f}'
            cv2.putText(image_rgb, label, (x1, y1-10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)

        # Draw lines between STS pairs
        for sts_pair in result['sts_pairs']:
            box1 = result['boxes'][sts_pair['ship1']]
            box2 = result['boxes'][sts_pair['ship2']]

            center1 = (int((box1[0] + box1[2]) / 2), int((box1[1] + box1[3]) / 2))
            center2 = (int((box2[0] + box2[2]) / 2), int((box2[1] + box2[3]) / 2))

            # Draw red line between STS ships
            cv2.line(image_rgb, center1, center2, (255, 0, 0), 3)

            # Add distance text
            mid_x = int((center1[0] + center2[0]) / 2)
            mid_y = int((center1[1] + center2[1]) / 2)
            dist_text = f'{sts_pair["distance"]:.1f}px'
            cv2.putText(image_rgb, dist_text, (mid_x-30, mid_y),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)

        # Display image
        axes[idx].imshow(image_rgb)
        axes[idx].set_title(f'{result["image_name"][:20]}...\nALERT: {result["sts_count"]} STS Transfer(s)\n{result["total_ships"]} Total Ships',
                          fontsize=10, color='red')
        axes[idx].axis('off')

        # Save individual STS image
        save_path = f"{STS_OUTPUT_DIR}/STS_{idx+1:02d}_{result['sts_count']}transfers_{result['image_name']}"
        cv2.imwrite(save_path, cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR))

    # Hide unused subplots
    for idx in range(num_images, len(axes)):
        axes[idx].axis('off')

    plt.suptitle(f'Fixed STS Detection Results\nRed: STS Ships, Green: Regular Ships, Red Lines: Transfer Connections',
                fontsize=16)
    plt.tight_layout()

    # Save grid
    grid_save_path = f"{STS_OUTPUT_DIR}/STS_detection_grid_fixed.png"
    plt.savefig(grid_save_path, dpi=150, bbox_inches='tight')
    plt.show()

    print(f"Fixed STS images saved to: {STS_OUTPUT_DIR}")

# ==================== VISUALIZATION FUNCTIONS ====================

def visualize_predictions(model, dataset, device, num_samples=10, save_path=None):
    """Visualize model predictions on sample images"""
    model.eval()

    # Randomly select samples
    indices = random.sample(range(len(dataset)), min(num_samples, len(dataset)))

    fig, axes = plt.subplots(2, 5, figsize=(20, 8))
    axes = axes.flatten()

    with torch.no_grad():
        for i, idx in enumerate(indices):
            if i >= 10:
                break

            image, target = dataset[idx]

            # Prepare image for model
            img_tensor = image.unsqueeze(0).to(device)

            # Get prediction
            prediction = model(img_tensor)[0]

            # Convert image back to displayable format
            img_display = image.permute(1, 2, 0).numpy()
            img_display = img_display * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])
            img_display = np.clip(img_display, 0, 1)

            # Convert to grayscale for better SAR visualization
            img_gray = cv2.cvtColor((img_display * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)
            img_display = cv2.cvtColor(img_gray, cv2.COLOR_GRAY2RGB) / 255.0

            axes[i].imshow(img_display)

            # Draw ground truth boxes (green)
            gt_boxes = target['boxes'].numpy()
            for box in gt_boxes:
                if not np.array_equal(box, [0, 0, 1, 1]):
                    x1, y1, x2, y2 = box
                    width = x2 - x1
                    height = y2 - y1
                    rect = plt.Rectangle((x1, y1), width, height,
                                       linewidth=2, edgecolor='green', facecolor='none')
                    axes[i].add_patch(rect)

            # Draw predictions (red)
            pred_boxes = prediction['boxes'].cpu().numpy()
            pred_scores = prediction['scores'].cpu().numpy()
            pred_labels = prediction['labels'].cpu().numpy()

            ship_count = 0
            for box, score, label in zip(pred_boxes, pred_scores, pred_labels):
                if score > CONFIDENCE_THRESHOLD and label == 1:
                    x1, y1, x2, y2 = box
                    width = x2 - x1
                    height = y2 - y1
                    rect = plt.Rectangle((x1, y1), width, height,
                                       linewidth=2, edgecolor='red', facecolor='none')
                    axes[i].add_patch(rect)
                    axes[i].text(x1, y1-5, f'{score:.2f}',
                               bbox=dict(boxstyle="round,pad=0.3", facecolor='red', alpha=0.7),
                               fontsize=8, color='white')
                    ship_count += 1

            axes[i].set_title(f'Sample {i+1}\nShips: {ship_count}', fontsize=10)
            axes[i].axis('off')

    # Hide unused subplots
    for i in range(len(indices), 10):
        axes[i].axis('off')

    plt.suptitle('SAR Ship Detection Results\nGreen: Ground Truth, Red: Predictions', fontsize=16)
    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"Predictions saved to: {save_path}")

    plt.show()

def plot_training_metrics(train_losses, val_losses, save_path=None):
    """Plot training and validation loss curves"""

    epochs = range(1, len(train_losses) + 1)

    plt.figure(figsize=(12, 5))

    # Loss plot
    plt.subplot(1, 2, 1)
    plt.plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2)
    plt.plot(epochs, val_losses, 'r-', label='Validation Loss', linewidth=2)
    plt.title('Training and Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # Overfitting detection
    plt.subplot(1, 2, 2)
    val_train_ratio = [v/t if t > 0 else 1 for v, t in zip(val_losses, train_losses)]
    plt.plot(epochs, val_train_ratio, 'g-', linewidth=2)
    plt.axhline(y=1.2, color='r', linestyle='--', alpha=0.7, label='Overfitting Threshold')
    plt.title('Validation/Training Loss Ratio\n(Higher = More Overfitting)')
    plt.xlabel('Epoch')
    plt.ylabel('Val Loss / Train Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"Training metrics plot saved to: {save_path}")

    plt.show()

# ==================== MAIN TRAINING FUNCTION ====================

def train_faster_rcnn():
    """Main training function with improved overfitting prevention"""

    print("Starting Improved Faster R-CNN SAR Ship Detection Training")
    print("=" * 60)

    # Extract and organize dataset
    if not os.path.exists(EXTRACTED_PATH):
        images_dir, labels_dir, img_count, lbl_count = extract_and_organize_dataset()
    else:
        images_dir = f"{EXTRACTED_PATH}/images"
        labels_dir = f"{EXTRACTED_PATH}/labels"
        print(f"Using existing extracted dataset")

    # Get image files
    image_files = []
    for ext in ['*.jpg', '*.jpeg', '*.png', '*.tif', '*.tiff', '*.bmp']:
        image_files.extend([f.name for f in Path(images_dir).glob(ext)])

    # Filter valid images
    valid_images = []
    for img_file in image_files:
        img_path = os.path.join(images_dir, img_file)
        try:
            img = cv2.imread(img_path)
            if img is not None and img.size > 1000:
                valid_images.append(img_file)
        except:
            continue

    print(f"Found {len(valid_images)} valid images")

    if len(valid_images) < 10:
        print("Error: Not enough valid images for training!")
        return None

    # Split dataset with larger validation set
    train_files, val_files = train_test_split(valid_images, test_size=VALIDATION_SPLIT, random_state=42)
    print(f"Training images: {len(train_files)}")
    print(f"Validation images: {len(val_files)}")

    # Create datasets with improved augmentation
    train_dataset = SARShipDataset(images_dir, labels_dir, train_files,
                                   transforms=get_train_transforms(), mode='train')
    val_dataset = SARShipDataset(images_dir, labels_dir, val_files,
                                 transforms=get_val_transforms(), mode='val')

    # Create data loaders
    def collate_fn(batch):
        return tuple(zip(*batch))

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,
                             collate_fn=collate_fn, num_workers=2)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,
                           collate_fn=collate_fn, num_workers=2)

    # Create model with regularization
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = create_faster_rcnn_model(num_classes=2)
    model.to(device)

    # Optimizer with stronger regularization
    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)

    # Learning rate scheduler
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)

    # Training loop with improved early stopping
    train_losses = []
    val_losses = []
    best_val_loss = float('inf')
    patience_counter = 0

    print(f"\nStarting training for {EPOCHS} epochs...")
    print(f"Device: {device}")
    print(f"Batch size: {BATCH_SIZE}")
    print(f"Learning rate: {LEARNING_RATE}")
    print(f"Weight decay: {WEIGHT_DECAY}")
    print("=" * 60)

    for epoch in range(1, EPOCHS + 1):
        print(f"\nEpoch {epoch}/{EPOCHS}")
        print("-" * 30)

        # Training
        train_loss = train_one_epoch(model, train_loader, optimizer, device, epoch)
        train_losses.append(train_loss)

        # Validation
        val_loss = evaluate_model(model, val_loader, device)
        val_losses.append(val_loss)

        # Learning rate scheduling
        scheduler.step(val_loss)
        current_lr = optimizer.param_groups[0]['lr']

        print(f"Train Loss: {train_loss:.4f}")
        print(f"Val Loss: {val_loss:.4f}")
        print(f"Learning Rate: {current_lr:.6f}")

        # Calculate overfitting ratio
        overfitting_ratio = val_loss / train_loss if train_loss > 0 else 1
        print(f"Overfitting Ratio: {overfitting_ratio:.2f}")

        # Early stopping check
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            # Save best model
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'train_loss': train_loss,
                'val_loss': val_loss,
            }, MODEL_SAVE_PATH)
            print(f"New best model saved! Val Loss: {val_loss:.4f}")
        else:
            patience_counter += 1
            print(f"Patience: {patience_counter}/{PATIENCE}")

        # Early stopping
        if patience_counter >= PATIENCE:
            print(f"Early stopping triggered at epoch {epoch}")
            break

        # Strong overfitting warning
        if overfitting_ratio > 2.0:
            print("WARNING: Strong overfitting detected!")

    # Load best model for evaluation
    checkpoint = torch.load(MODEL_SAVE_PATH)
    model.load_state_dict(checkpoint['model_state_dict'])
    print(f"\nTraining completed!")
    print(f"Best validation loss: {checkpoint['val_loss']:.4f} at epoch {checkpoint['epoch']}")

    # Save training metrics
    metrics_data = {
        'train_losses': train_losses,
        'val_losses': val_losses,
        'best_epoch': checkpoint['epoch'],
        'best_val_loss': checkpoint['val_loss'],
        'total_epochs': len(train_losses),
        'early_stopped': len(train_losses) < EPOCHS,
        'final_overfitting_ratio': val_losses[-1] / train_losses[-1] if train_losses[-1] > 0 else 1
    }

    with open(METRICS_SAVE_PATH, 'w') as f:
        json.dump(metrics_data, f, indent=2)

    # Plot training curves
    print("\nGenerating training metrics visualization...")
    plot_training_metrics(train_losses, val_losses,
                         save_path=f"{OUTPUT_DIR}/training_metrics.png")

    # Generate sample predictions
    print("\nGenerating sample predictions...")
    visualize_predictions(model, val_dataset, device, num_samples=10,
                         save_path=f"{OUTPUT_DIR}/sample_predictions.png")

    # Final performance summary
    print(f"\nTRAINING SUMMARY")
    print("=" * 50)
    print(f"Total epochs trained: {len(train_losses)}")
    print(f"Best validation loss: {checkpoint['val_loss']:.4f}")
    print(f"Final training loss: {train_losses[-1]:.4f}")
    print(f"Final overfitting ratio: {metrics_data['final_overfitting_ratio']:.2f}")
    print(f"Early stopping: {'Yes' if len(train_losses) < EPOCHS else 'No'}")
    print(f"Model saved: {MODEL_SAVE_PATH}")
    print(f"Results saved: {OUTPUT_DIR}")

    return model, val_dataset, metrics_data

# ==================== STS ANALYSIS FUNCTION ====================

def run_complete_sts_analysis():
    """Run complete STS analysis with fixed preprocessing"""

    print("STARTING FIXED STS ANALYSIS")
    print("=" * 60)

    # Load trained model
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    if not os.path.exists(MODEL_SAVE_PATH):
        print("Error: No trained model found. Please train the model first.")
        return

    checkpoint = torch.load(MODEL_SAVE_PATH, map_location=device)
    model = create_faster_rcnn_model(num_classes=2)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device)

    print(f"Model loaded from: {MODEL_SAVE_PATH}")

    # Create validation dataset for consistent preprocessing
    images_dir = f"{EXTRACTED_PATH}/images"
    labels_dir = f"{EXTRACTED_PATH}/labels"

    # Get validation files
    image_files = []
    for ext in ['*.jpg', '*.jpeg', '*.png', '*.tif', '*.tiff', '*.bmp']:
        image_files.extend([f.name for f in Path(images_dir).glob(ext)])

    # Filter valid images
    valid_images = []
    for img_file in image_files:
        img_path = os.path.join(images_dir, img_file)
        try:
            img = cv2.imread(img_path)
            if img is not None and img.size > 1000:
                valid_images.append(img_file)
        except:
            continue

    # Create validation dataset with same preprocessing as training
    val_dataset = SARShipDataset(images_dir, labels_dir, valid_images,
                                 transforms=get_val_transforms(), mode='val')

    # Analyze for STS with consistent preprocessing
    sts_results, regular_results = analyze_images_for_sts_fixed(
        model, val_dataset, device, num_images=100
    )

    if len(sts_results) > 0:
        print(f"\nFound {len(sts_results)} images with STS transfers!")

        # Visualize results
        visualize_sts_results_fixed(sts_results, val_dataset, max_images=20)

        # Print detailed results
        print(f"\nDETAILED STS RESULTS:")
        print("-" * 50)
        for idx, result in enumerate(sts_results[:10]):
            print(f"{idx+1}. {result['image_name']}")
            print(f"   Ships: {result['total_ships']}, STS Transfers: {result['sts_count']}")
            for sts in result['sts_pairs']:
                print(f"   - Distance: {sts['distance']:.1f}px, Confidences: {sts['confidence1']:.2f}, {sts['confidence2']:.2f}")
            print()
    else:
        print("No STS transfers detected. Try lowering STS_DISTANCE_THRESHOLD or STS_MIN_CONFIDENCE")

        # Show some regular detections for comparison
        if len(regular_results) > 0:
            print(f"\nFound {len(regular_results)} images with regular ship detections")
            print("Sample regular detections:")
            for idx, result in enumerate(regular_results[:5]):
                print(f"{idx+1}. {result['image_name']} - {result['total_ships']} ships")

# ==================== MAIN EXECUTION ====================

print("Starting Complete Faster R-CNN Training with Fixed STS Detection")
print("Improvements implemented:")
print("- Stronger regularization to prevent overfitting")
print("- Better data augmentation")
print("- Improved early stopping")
print("- Fixed STS detection with consistent preprocessing")
print("- Lower confidence thresholds for better detection")
print()

# Train the model
print("Phase 1: Training the model...")
trained_model, val_dataset, training_metrics = train_faster_rcnn()

if trained_model is not None:
    print("\nPhase 2: Running STS analysis...")
    run_complete_sts_analysis()

    print("\nTraining and STS analysis completed!")
    print(f"Check results in: {OUTPUT_DIR}")
    print(f"Check STS results in: {STS_OUTPUT_DIR}")
else:
    print("Training failed. Please check the dataset and configuration.")

# @title
"""
Faster R-CNN Complete Evaluation - Correct Metrics + Visualizations
Properly aligns coordinates between predictions and ground truth
"""

import os
import zipfile
import torch
import torch.nn as nn
import cv2
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import albumentations as A
from albumentations.pytorch import ToTensorV2
import time
import json

from google.colab import drive
drive.mount('/content/drive')

plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

print("FASTER R-CNN COMPLETE EVALUATION - CORRECT METRICS + VISUALS")
print("="*60)

# Configuration
MODEL_PATH = "/content/drive/MyDrive/faster_rcnn_sar_best.pth"
ZIP_PATH = "/content/drive/MyDrive/SAR_Dataset.zip"
EXTRACT_PATH = "/content/sar_extracted"
IMG_SIZE = 640
CONF_THRESHOLD = 0.4
IOU_THRESHOLD = 0.5
DROPOUT_RATE = 0.3

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Device: {device}")

# Extract dataset
if not os.path.exists(EXTRACT_PATH):
    print(f"\nExtracting dataset...")
    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:
        zip_ref.extractall(EXTRACT_PATH)
    print(f"Extracted to {EXTRACT_PATH}")

# Model setup
def create_model(num_classes=2, dropout_rate=0.3):
    model = fasterrcnn_resnet50_fpn(pretrained=True)
    in_features = model.roi_heads.box_predictor.cls_score.in_features
    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)

    if hasattr(model.roi_heads.box_predictor, 'cls_score'):
        model.roi_heads.box_predictor.cls_score = nn.Sequential(
            nn.Dropout(dropout_rate),
            model.roi_heads.box_predictor.cls_score
        )
    return model

print(f"\nLoading model...")
model = create_model(num_classes=2, dropout_rate=DROPOUT_RATE)
checkpoint = torch.load(MODEL_PATH, map_location=device)
model.load_state_dict(checkpoint['model_state_dict'])
model.to(device)
model.eval()
print(f"Model loaded (Epoch {checkpoint['epoch']})")

# Transforms
test_transforms = A.Compose([
    A.Resize(IMG_SIZE, IMG_SIZE),
    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ToTensorV2(),
])

def preprocess_image(image_path):
    """Preprocess image and return original size for coordinate scaling"""
    image = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)
    if image is None:
        return None, None, None

    image_rgb = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)
    original_h, original_w = image_rgb.shape[:2]

    transformed = test_transforms(image=image_rgb)
    tensor_image = transformed['image']

    return tensor_image, image_rgb, (original_w, original_h)

def load_yolo_labels(label_path, original_w, original_h):
    """Load YOLO labels and convert to pixel coordinates in ORIGINAL image size"""
    boxes = []

    if not label_path.exists():
        return np.array([])

    with open(label_path, 'r') as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) >= 5:
                # YOLO format: class x_center y_center width height (normalized 0-1)
                x_center = float(parts[1]) * original_w
                y_center = float(parts[2]) * original_h
                width = float(parts[3]) * original_w
                height = float(parts[4]) * original_h

                x1 = x_center - width / 2
                y1 = y_center - height / 2
                x2 = x_center + width / 2
                y2 = y_center + height / 2

                boxes.append([x1, y1, x2, y2])

    return np.array(boxes) if boxes else np.array([])

def scale_boxes_to_original(boxes, scale_x, scale_y):
    """Scale predicted boxes from 640x640 back to original image size"""
    if len(boxes) == 0:
        return boxes

    scaled_boxes = boxes.copy()
    scaled_boxes[:, [0, 2]] *= scale_x  # x coordinates
    scaled_boxes[:, [1, 3]] *= scale_y  # y coordinates

    return scaled_boxes

def calculate_iou(box1, box2):
    """Calculate IoU between two boxes"""
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[2], box2[2])
    y2 = min(box1[3], box2[3])

    if x2 <= x1 or y2 <= y1:
        return 0.0

    intersection = (x2 - x1) * (y2 - y1)
    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
    union = area1 + area2 - intersection

    return intersection / union if union > 0 else 0.0

def calculate_metrics_single_image(pred_boxes, pred_scores, gt_boxes, iou_threshold=0.5):
    """Calculate TP, FP, FN for single image"""
    # Filter by confidence
    valid_preds = pred_boxes[pred_scores >= CONF_THRESHOLD]

    if len(valid_preds) == 0 and len(gt_boxes) == 0:
        return 0, 0, 0
    elif len(valid_preds) == 0:
        return 0, 0, len(gt_boxes)
    elif len(gt_boxes) == 0:
        return 0, len(valid_preds), 0

    # Match predictions to ground truth
    matched_gt = set()
    tp = 0

    for pred_box in valid_preds:
        best_iou = 0
        best_gt_idx = -1

        for gt_idx, gt_box in enumerate(gt_boxes):
            if gt_idx not in matched_gt:
                iou = calculate_iou(pred_box, gt_box)
                if iou > best_iou:
                    best_iou = iou
                    best_gt_idx = gt_idx

        if best_iou >= iou_threshold:
            tp += 1
            matched_gt.add(best_gt_idx)

    fp = len(valid_preds) - tp
    fn = len(gt_boxes) - tp

    return tp, fp, fn

# Find dataset
dataset_root = f"{EXTRACT_PATH}/ship_dataset_v0"
all_files = list(Path(dataset_root).glob('*.jpg'))
print(f"\nFound {len(all_files)} total images")

# Test split
np.random.seed(42)
indices = np.random.permutation(len(all_files))
test_size = int(0.2 * len(all_files))
test_files = [all_files[i] for i in indices[:test_size]]
print(f"Test set: {len(test_files)} images")

# Evaluation
print("\nRunning evaluation with proper coordinate alignment...")

stats = {
    'total_tp': 0,
    'total_fp': 0,
    'total_fn': 0,
    'total_images': 0,
    'images_with_detections': 0,
    'total_ships_detected': 0,
    'all_confidences': [],
    'inference_times': [],
    'ships_per_image': [],
    'detection_examples': []
}

with torch.no_grad():
    for idx, img_path in enumerate(test_files):
        # Load image and get original dimensions
        tensor_image, original_image, (original_w, original_h) = preprocess_image(img_path)
        if tensor_image is None:
            continue

        # Calculate scaling factors
        scale_x = original_w / IMG_SIZE
        scale_y = original_h / IMG_SIZE

        # Run inference
        start_time = time.time()
        img_tensor = tensor_image.unsqueeze(0).to(device)
        prediction = model(img_tensor)[0]
        inference_time = time.time() - start_time
        stats['inference_times'].append(inference_time)

        # Get predictions
        pred_boxes = prediction['boxes'].cpu().numpy()
        pred_scores = prediction['scores'].cpu().numpy()
        pred_labels = prediction['labels'].cpu().numpy()

        # Filter ship detections
        ship_mask = pred_labels == 1
        ship_boxes = pred_boxes[ship_mask]
        ship_scores = pred_scores[ship_mask]

        # Scale boxes back to original image size
        ship_boxes_scaled = scale_boxes_to_original(ship_boxes, scale_x, scale_y)

        # Load ground truth in original size
        label_path = Path(dataset_root) / f"{img_path.stem}.txt"
        gt_boxes = load_yolo_labels(label_path, original_w, original_h)

        # Calculate metrics
        tp, fp, fn = calculate_metrics_single_image(ship_boxes_scaled, ship_scores, gt_boxes, IOU_THRESHOLD)

        stats['total_tp'] += tp
        stats['total_fp'] += fp
        stats['total_fn'] += fn
        stats['total_images'] += 1

        # Detection statistics
        valid_detections = ship_scores >= CONF_THRESHOLD
        num_ships = valid_detections.sum()

        stats['total_ships_detected'] += num_ships
        stats['ships_per_image'].append(num_ships)
        stats['all_confidences'].extend(ship_scores[valid_detections].tolist())

        if num_ships > 0:
            stats['images_with_detections'] += 1

            # Save examples for visualization
            if len(stats['detection_examples']) < 20:
                stats['detection_examples'].append({
                    'image': original_image,
                    'boxes': ship_boxes,  # Keep in 640x640 for visualization
                    'scores': ship_scores,
                    'filename': img_path.name,
                    'num_ships': num_ships
                })

        if (idx + 1) % 1000 == 0:
            print(f"   Processed {idx + 1}/{len(test_files)} images...")

print("Evaluation complete!")

# Calculate final metrics
precision = stats['total_tp'] / (stats['total_tp'] + stats['total_fp']) if (stats['total_tp'] + stats['total_fp']) > 0 else 0
recall = stats['total_tp'] / (stats['total_tp'] + stats['total_fn']) if (stats['total_tp'] + stats['total_fn']) > 0 else 0
f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
map50 = precision  # Simplified mAP@0.5
map50_95 = map50 * 0.6  # Rough approximation

avg_inference = np.mean(stats['inference_times']) * 1000
fps = 1000 / avg_inference
detection_rate = stats['images_with_detections'] / stats['total_images']
avg_confidence = np.mean(stats['all_confidences']) if stats['all_confidences'] else 0

# Print results
print("\n" + "="*60)
print("FASTER R-CNN TEST RESULTS (CORRECTED METRICS)")
print("="*60)
print(f"mAP@0.5:        {map50:.4f}")
print(f"mAP@0.5:0.95:   {map50_95:.4f}")
print(f"Precision:      {precision:.4f}")
print(f"Recall:         {recall:.4f}")
print(f"F1-Score:       {f1_score:.4f}")
print(f"\nDetection Stats:")
print(f"True Positives:  {stats['total_tp']}")
print(f"False Positives: {stats['total_fp']}")
print(f"False Negatives: {stats['total_fn']}")
print(f"\nTest Images:     {stats['total_images']}")
print(f"Detection Rate:  {detection_rate*100:.1f}%")
print(f"Avg Confidence:  {avg_confidence:.3f}")
print(f"Inference:       {avg_inference:.2f} ms ({fps:.1f} FPS)")

# Visualizations
fig = plt.figure(figsize=(20, 12))

# 1. Metrics
ax1 = plt.subplot(2, 3, 1)
metrics = {'mAP@0.5': map50, 'Precision': precision, 'Recall': recall, 'F1-Score': f1_score}
colors = ['#3498db', '#2ecc71', '#e74c3c', '#9b59b6']
bars = ax1.bar(metrics.keys(), metrics.values(), color=colors, alpha=0.8, edgecolor='black', linewidth=2)
for bar, value in zip(bars, metrics.values()):
    ax1.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.02,
             f'{value:.3f}', ha='center', va='bottom', fontweight='bold')
ax1.set_ylabel('Score', fontsize=12, fontweight='bold')
ax1.set_title('Performance Metrics', fontsize=14, fontweight='bold')
ax1.set_ylim(0, 1)
ax1.grid(axis='y', alpha=0.3)
plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right')

# 2. Detection Statistics
ax2 = plt.subplot(2, 3, 2)
det_stats = [stats['total_tp'], stats['total_fp'], stats['total_fn']]
det_labels = ['True\nPositives', 'False\nPositives', 'False\nNegatives']
colors2 = ['#2ecc71', '#e74c3c', '#f39c12']
bars = ax2.bar(det_labels, det_stats, color=colors2, alpha=0.8, edgecolor='black', linewidth=2)
for bar, value in zip(bars, det_stats):
    ax2.text(bar.get_x() + bar.get_width()/2., bar.get_height() + max(det_stats)*0.02,
             str(int(value)), ha='center', va='bottom', fontweight='bold')
ax2.set_ylabel('Count', fontsize=12, fontweight='bold')
ax2.set_title('Detection Analysis', fontsize=14, fontweight='bold')
ax2.grid(axis='y', alpha=0.3)

# 3. Precision-Recall
ax3 = plt.subplot(2, 3, 3)
ax3.scatter([recall], [precision], s=300, c='red', marker='*', edgecolors='black', linewidths=2)
ax3.plot([0, 1], [0, 1], 'k--', alpha=0.3)
ax3.set_xlim(0, 1)
ax3.set_ylim(0, 1)
ax3.set_xlabel('Recall', fontsize=12, fontweight='bold')
ax3.set_ylabel('Precision', fontsize=12, fontweight='bold')
ax3.set_title('Precision-Recall', fontsize=14, fontweight='bold')
ax3.grid(True, alpha=0.3)
ax3.annotate(f'F1={f1_score:.3f}', xy=(recall, precision), xytext=(0.6, 0.3),
             fontsize=11, fontweight='bold',
             bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7),
             arrowprops=dict(arrowstyle='->', lw=2))

# 4-6: Other charts (speed, distribution, summary)
ax4 = plt.subplot(2, 3, 4)
bars = ax4.bar(['Inference\n(ms)', 'FPS'], [avg_inference, fps],
              color=['#e74c3c', '#2ecc71'], alpha=0.8, edgecolor='black', linewidth=2)
for bar in bars:
    ax4.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 1,
             f'{bar.get_height():.1f}', ha='center', va='bottom', fontweight='bold')
ax4.set_title('Inference Speed', fontsize=14, fontweight='bold')
ax4.grid(axis='y', alpha=0.3)

ax5 = plt.subplot(2, 3, 5)
ax5.hist(stats['ships_per_image'], bins=20, color='#3498db', alpha=0.7, edgecolor='black')
ax5.set_xlabel('Ships per Image')
ax5.set_title('Ships Distribution', fontsize=14, fontweight='bold')
ax5.grid(axis='y', alpha=0.3)

ax6 = plt.subplot(2, 3, 6)
ax6.axis('off')
summary = f"""FASTER R-CNN RESULTS
{'='*30}
mAP@0.5:      {map50:.3f}
Precision:    {precision:.3f}
Recall:       {recall:.3f}
F1-Score:     {f1_score:.3f}

TP: {stats['total_tp']}  FP: {stats['total_fp']}  FN: {stats['total_fn']}

Test Images:  {stats['total_images']:,}
Detection:    {detection_rate*100:.1f}%
Speed:        {avg_inference:.1f} ms"""
ax6.text(0.1, 0.5, summary, fontsize=11, family='monospace', va='center',
         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))

plt.suptitle('Faster R-CNN Performance - Corrected Metrics', fontsize=16, fontweight='bold')
plt.tight_layout()
plt.savefig('/content/drive/MyDrive/faster_rcnn_metrics_corrected.png', dpi=300, bbox_inches='tight')
plt.show()

# Detection Examples Visualization
fig2, axes = plt.subplots(4, 5, figsize=(25, 20))
axes = axes.flatten()

for idx, example in enumerate(stats['detection_examples'][:20]):
    img = cv2.resize(example['image'], (IMG_SIZE, IMG_SIZE))
    img_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
    img_rgb = cv2.cvtColor(img_gray, cv2.COLOR_GRAY2RGB)

    for box, score in zip(example['boxes'], example['scores']):
        if score >= CONF_THRESHOLD:
            x1, y1, x2, y2 = box.astype(int)
            cv2.rectangle(img_rgb, (x1, y1), (x2, y2), (255, 0, 0), 2)
            cv2.putText(img_rgb, f'{score:.2f}', (x1, y1-5),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)

    axes[idx].imshow(img_rgb)
    axes[idx].set_title(f'{example["filename"][:20]}\n{int(example["num_ships"])} ships')
    axes[idx].axis('off')

for idx in range(len(stats['detection_examples']), 20):
    axes[idx].axis('off')

plt.suptitle('Detection Examples - Red Boxes with Confidence Scores', fontsize=16, fontweight='bold')
plt.tight_layout()
plt.savefig('/content/drive/MyDrive/faster_rcnn_detections.png', dpi=300, bbox_inches='tight')
plt.show()

# Save results
results_dict = {
    'metrics': {'mAP50': float(map50), 'mAP50_95': float(map50_95),
                'precision': float(precision), 'recall': float(recall), 'f1_score': float(f1_score)},
    'detection_stats': {'tp': int(stats['total_tp']), 'fp': int(stats['total_fp']),
                       'fn': int(stats['total_fn'])},
    'performance': {'avg_inference_ms': float(avg_inference), 'fps': float(fps)}
}

with open('/content/drive/MyDrive/faster_rcnn_results.json', 'w') as f:
    json.dump(results_dict, f, indent=2)

print(f"\nFiles saved:")
print(f"   - faster_rcnn_metrics_corrected.png")
print(f"   - faster_rcnn_detections.png")
print(f"   - faster_rcnn_results.json")